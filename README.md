# Sign Language to Text and Speech
Overview

This project enables real-time conversion of American Sign Language (ASL) gestures into text and speech. Using a CNN-LSTM model trained on the WLASL dataset, it captures video input, recognizes signs, and outputs corresponding text and audio. The system features a Flask-based web interface for user interaction, supporting accessibility for the deaf and hard-of-hearing community.

Features





Real-Time ASL Detection: Processes video streams to identify ASL gestures.



Text Output: Displays recognized signs as text.



Speech Synthesis: Converts text to speech using gTTS.



Web Interface: User-friendly UI with pages for Home, Conversion, About, and Contact.



Responsive Design: Hamburger menu and square image cards for features and team profiles.
